---
layout: post
title: "熵、相对熵和互信息"
date: 2015-04-20 19:56:13 +0800
comments: true
categories: 信息论, 熵
---

综述熵、相对熵和互信息的概念

<!--more-->

熵
===

**定义** 离散型随机变量X的*熵*(entropy) $H(X)$定义为：

$$ H(X) = -\sum_{x}{p(x)\log{p(x)}} $$

> 注意：熵实际上是随机变量$X$概率分布的泛函

联合熵和条件熵
============

**定义** 对于服从联合分布为$p(x,y)$的一对离散型随机变量$(X,Y)$，其*联合熵*(joint entropy) $H(X,Y)$定义为：

$$ H(X,Y) = -\sum_{x}{\sum_{y}{p(x,y)\log{p(x,y)}}} $$

亦可表示成：

$$ H(X,Y) = -E\log{p(X,Y)} $$

即一个随机变量在给定另一个随机变量下的条件熵，它是条件分布熵关于起条件作用的那个随机变量取平均后的期望值。

**定义** 若$(X,Y)$服从联合分布$p(x,y)$,*条件熵*(conditional entropy) $H(Y|X)$定义为：

$$ H(Y|X) = \sum_{x}{p(x)H(Y|X=x)} $$

**链式法则** 
$$ H(X,Y) = H(X) + H(Y|X) $$