---
layout: post
title: "基于梯度的最优化算法——共轭梯度法"
date: 2015-05-20 16:52:49 +0800
comments: true
categories: 机器学习, 模式识别, 最优化
---

在模式识别与机器学习中，经常需要求解目标函数最优化问题（往往是函数极值问题）。而根据问题的性质可将求解算法分为如下两大类：

+ 求解`无约束确定性最优化`问题 —— 基于梯度的算法
+ 求解`等式约束最优化`问题 —— Larange乘子法
+ 求解`随机性`问题 —— 随机逼近算法

本系列“基于梯度的最优化算法”介绍三种基于梯度的无约束最优化问题求解算法。本文首先介绍最简单也是最常用的`梯度下降法`。

<!-- more -->

算法思想
=======

假设我们要求的是某准则函数$J(a)$的极小值（显然极大值问题可通过给函数添加一个符号来变成极小值问题）。

由微积分我们知道函数$J(a)$在某点$a_k$的梯度**$\nabla{J(a_k)}$是一个向量**，其**方向是函数增长最快的方向**。所以，负梯度方向则是函数**减少最快的方向**。因此，如果我们一直沿着负梯度走，我们就能找到函数的极小值点。

该思想的另一种表述是，我们已知当$a_{k+1}-a_{k}$趋近于0时，有如下关系式成立：

$$ J(a_{k+1}) \approx J(a_k) + \nabla{J^T(a_{k+1} - a_k)} $$

因此，我们只需选择一个序列$\{a_k\}$使得$\nabla{J^T(a_{k+1} - a_k)} \leq 0$就会让序列$\{J(a)\}$不断减小，直至收敛至极小值点。

显然，若$a_{k+1}-a_{k} = -\rho_k\nabla{J(a_k)}$即可，其中$\rho_k$是一个很小的数，称为搜索步长。

因此，我们只需按下式构造序列$\{a_k\}$：

$$ a_{k+1} = -\rho_k\nabla{J(a_k)} $$

则可得到序列$\{J(a)\}$，该序列可收敛至函数极小点

> 注意：一般来讲这样只会找到局部极小点，但是实际中往往$J(a)$是凸函数，因此可收敛至全局极小点

算法描述
=======

1. 选取初始点$a_0$，设定收敛阈值$\epsilon>0,\eta>0$，并令$k=0$
2. 计算搜索方向$s_k=-\frac{\nabla{J(a_k)}}{\|\nabla{J(a_k)}\|}$
3. 如果$\|s_k\|\leq\eta$则输出结果并结束算法，否则继续
4. 计算搜索步长$\rho_k$（往往为简单起见可设为一小常数或者$k^{-1}$，**但是这里大有改进的空间**）
5. 令$a_{k+1} = a_{k}+\rho_ks_k$
6. 如果$J(a_{k+1}) - J(a_k) \leq \epsilon$则结束算法并停机，否则令$k = k+1$并转2

改进的可能
========

显然，我们上式只使用了函数的一阶导数，何不通过引入更高阶导数来提高计算效率与准确度呢？因此就有了牛顿法